{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# import logging\n",
    "# logger = logging.getLogger('root')\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete(path):\n",
    "    if (os.path.islink(path)):\n",
    "        os.unlink(path)\n",
    "    elif (os.path.isdir(path)):\n",
    "        for x in os.listdir(path):\n",
    "            delete(path + '/' + x)\n",
    "        os.rmdir(path)\n",
    "    else:\n",
    "        assert(os.path.isfile(path))\n",
    "        os.remove(path)\n",
    "\n",
    "def clear(path):\n",
    "    cnt = 0\n",
    "    if (os.path.islink(path)):\n",
    "        delete(path)\n",
    "    elif (os.path.isdir(path)):\n",
    "        for x in os.listdir(path):\n",
    "            if (x[0] == '.'):\n",
    "                delete(path + '/' + x)\n",
    "            else:\n",
    "                cnt += clear(path + '/' + x)\n",
    "        if (cnt == 0):\n",
    "            delete(path)\n",
    "    else:\n",
    "        assert(os.path.isfile(path))\n",
    "        if (path[-3:] == '.py'):\n",
    "            cnt = 1\n",
    "        else:\n",
    "            delete(path)\n",
    "    return cnt\n",
    "\n",
    "def _download_repo(repo_full_name, token, save_path):\n",
    "#     print(f\"git clone https://{token}@github.com.cnpmjs.org/{repo_full_name} {save_path}/{repo_full_name} --progress --depth 1\")\n",
    "    os.system(f\"git clone https://{token}@github.com.cnpmjs.org/{repo_full_name} {save_path}/{repo_full_name} --quiet --depth 1\")\n",
    "#     logger.info(f'Downloaded {repo_full_name}')\n",
    "    clear(save_path + '/' + repo_full_name)\n",
    "\n",
    "def download_repos(repo_full_names, tokens, save_path, num_process = 1):\n",
    "    pool = Pool(num_process)\n",
    "    for repo_full_name in repo_full_names:\n",
    "        token = random.choice(tokens)\n",
    "        pool.apply_async(_download_repo, args = (repo_full_name, token, save_path))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "# def download_repos(repo_full_names, tokens, save_path, num_process = 1):\n",
    "#     for repo_full_name in tqdm(repo_full_names):\n",
    "#         token = random.choice(tokens)\n",
    "#         _download_repo(repo_full_name, token, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_code_tokens_and_functions(path):\n",
    "    code_tokens = []\n",
    "    function_stack = []\n",
    "    code_functions = []\n",
    "    indent_number = 0\n",
    "    with open(path, 'rb') as input_file:\n",
    "        tokenGenerator = tokenize.tokenize(input_file.readline)\n",
    "        for token in tokenGenerator:\n",
    "            if (token.type in [0, 59, 60, 62]): # COMMENT\n",
    "                pass\n",
    "            elif (token.type in [4, 61]): # NEWLINE\n",
    "                pass\n",
    "            elif (token.type == 5): # INDENT\n",
    "#                 code_tokens.append('{')\n",
    "                indent_number += 1\n",
    "            elif (token.type == 6): # DEDENT\n",
    "#                 code_tokens.append('}')\n",
    "                indent_number -= 1\n",
    "                if (function_stack != [] and indent_number == function_stack[-1][3]):\n",
    "                    code_functions.append(function_stack.pop())\n",
    "                    code_functions[-1][2] = len(code_tokens)\n",
    "            elif (token.type in [1, 2, 3, 54]): # NAME NUMBER STRING OP\n",
    "                code_tokens.append(token.string)\n",
    "                if (token.string in ['def', 'class']):\n",
    "                    function_stack.append([0, len(code_tokens) - 1, 0, indent_number])\n",
    "                elif (function_stack != [] and function_stack[-1][0] == 0):\n",
    "                    function_stack[-1][0] = token.string\n",
    "            else:\n",
    "                assert(False)\n",
    "    return code_tokens, code_functions\n",
    "\n",
    "def _read_codes(repo_path, current_path = '.'):\n",
    "    data = {}\n",
    "    path = os.path.join(repo_path, current_path)\n",
    "    if (os.path.isdir(path)):\n",
    "        for x in os.listdir(path):\n",
    "            data.update(_read_codes(repo_path, current_path + '/' + x))\n",
    "    elif os.path.isfile(path):\n",
    "        if (path[-3 :] != '.py'):\n",
    "            return {}\n",
    "        try:\n",
    "            data[current_path] = _read_code_tokens_and_functions(path)\n",
    "        except:\n",
    "            f = open('badfile.txt', 'a')\n",
    "            print(path, file = f)\n",
    "            f.close()\n",
    "            return {}\n",
    "    else:\n",
    "        return {}\n",
    "    return data\n",
    "\n",
    "def read_repos(repos, filepath, num_process = 1):\n",
    "    dataset = []\n",
    "    pool = Pool(processes = num_process)\n",
    "    for _, repo in enumerate(repos):\n",
    "        dataset.append(pool.apply_async(_read_codes, (filepath + '/' + repo, '.')))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i] = dataset[i].get()\n",
    "    return dataset\n",
    "\n",
    "# def read_repos(repos, filepath, num_process = 1):\n",
    "#     dataset = []\n",
    "#     for _, repo in enumerate(repos):\n",
    "#         dataset.append(_read_codes(filepath + '/' + repo, '.'))\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeFeatureWorker():\n",
    "    def __init__(self, cfg):\n",
    "        self.save_path = cfg['save_path']\n",
    "        if os.path.exists(self.save_path) == False: os.mkdir(self.save_path)\n",
    "\n",
    "        self.keep_code = cfg['keep_code']\n",
    "        self.github_tokens = cfg['github_tokens']\n",
    "        self.num_process = cfg['num_process']\n",
    "        self.device = cfg['device']\n",
    "\n",
    "        self._init_model(cfg['model_cfg']) # TODO\n",
    "    \n",
    "    def _init_model(self, cfg):\n",
    "        self.hidden_dim = cfg['hidden_dim']\n",
    "        self.model_config = AutoConfig.from_pretrained(cfg['code_model'])\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cfg['code_model'])\n",
    "        self.code_model = AutoModel.from_pretrained(cfg['code_model']).to(self.device)\n",
    "        for param in self.code_model.parameters():\n",
    "            param.requires_grad = cfg['code_trainable']\n",
    "\n",
    "        self.rnn = nn.LSTM(self.hidden_dim, self.hidden_dim, num_layers = 1, batch_first = True).to(self.device)\n",
    "        for param in self.rnn.parameters():\n",
    "            param.requires_grad = cfg['rnn_trainable']\n",
    "\n",
    "    def _if_downloaded(self, repo_names):\n",
    "        return np.array([\n",
    "            os.path.exists(\n",
    "                os.path.join(self.save_path, repo_name)\n",
    "            ) for repo_name in repo_names\n",
    "        ])\n",
    "    \n",
    "    def _aggregate_code_embs(self, file_names, code_embs):\n",
    "        # Aggregate code emb of files to obtain repo emb\n",
    "        # Currently, file_names is not used.\n",
    "        code_embs = nn.utils.rnn.pad_sequence(code_embs, batch_first = True)\n",
    "        res, (_, _) = self.rnn(code_embs)\n",
    "        return res[:, -1, :]\n",
    "\n",
    "#     def _compute_embs(self, repo_codes):\n",
    "#         code_embs = []\n",
    "#         for repo in repo_codes:\n",
    "#             keys, values = list(repo.keys()), list(repo.values())\n",
    "#             # compute code emb from files\n",
    "#             tokens = self.tokenizer(values, return_tensors=\"pt\", truncation=True, padding='max_length').to(self.device)\n",
    "#             code_embs.append(self.code_model(**tokens)['pooler_output'])\n",
    "\n",
    "#         # aggregate embs of all files to obtain repo emb\n",
    "#         repo_embs = self._aggregate_code_embs(keys, code_embs)\n",
    "\n",
    "#         return repo_embs\n",
    "    \n",
    "    def _compute_embs2(self, repo_codes, batch_size = 8):\n",
    "        repo_embs = []\n",
    "        for repo in repo_codes:\n",
    "            keys, values = list(repo.keys()), list(repo.values())\n",
    "            repo_size = len(values)\n",
    "            if (repo_size == 0 or repo_size > 2000):\n",
    "                repo_embs.append(0)\n",
    "                continue\n",
    "            # compute code emb from files\n",
    "            tokens = self.tokenizer(values, return_tensors=\"pt\", truncation=True, padding='max_length').to(self.device)\n",
    "            \n",
    "            code_embs = []\n",
    "            for i in range(0, repo_size, batch_size):\n",
    "                end_index = min(repo_size, i + batch_size)\n",
    "                code_embs.append(self.code_model(input_ids = tokens['input_ids'][i : end_index],\n",
    "                                                 attention_mask = tokens['attention_mask'][i : end_index])['pooler_output'])\n",
    "            code_embs = [torch.cat(code_embs, dim = 0)]\n",
    "            \n",
    "            # aggregate embs of all files to obtain repo emb\n",
    "            repo_embs.append(self._aggregate_code_embs(keys, code_embs).view(-1).tolist())\n",
    "        \n",
    "        return repo_embs\n",
    "    \n",
    "    def _update_model_and_tokenizer(self, model, tokenizer, func_names, func_contexts, batch_size):\n",
    "        # update tokenizer\n",
    "        _ = tokenizer.add_tokens(func_names)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        # compute func embs\n",
    "        context_tokens = tokenizer(func_contexts, return_tensors=\"pt\",\n",
    "                                   truncation=True, padding='max_length').to(self.device)\n",
    "        print(context_tokens['input_ids'].shape)\n",
    "        \n",
    "#         func_embs = model(**context_tokens)['pooler_output']\n",
    "        func_embs = []\n",
    "        print(len(func_names))\n",
    "        for i in range(0, len(func_names), batch_size):\n",
    "            end_index = min(len(func_names), i + batch_size)\n",
    "            func_embs.append(\n",
    "                model(input_ids = context_tokens['input_ids'][i : end_index],\n",
    "                      attention_mask = context_tokens['attention_mask'][i : end_index])\n",
    "                ['pooler_output']) # 就是这句话爆显存\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        func_embs = torch.cat(func_embs, dim = 0)\n",
    "        print(func_embs)\n",
    "        \n",
    "#         code_embs = []\n",
    "#         for i in range(0, repo_size, batch_size):\n",
    "#             end_index = min(repo_size, i + batch_size)\n",
    "#             code_embs.append(self.code_model(input_ids = tokens['input_ids'][i : end_index],\n",
    "#                                              attention_mask = tokens['attention_mask'][i : end_index])['pooler_output'])\n",
    "#         code_embs = [torch.cat(code_embs, dim = 0)]\n",
    "        \n",
    "        \n",
    "        func_token_ids = tokenizer.convert_tokens_to_ids(func_names)\n",
    "        with torch.no_grad():\n",
    "            model.embeddings.word_embeddings.weight[func_token_ids] = func_embs\n",
    "        return model, tokenizer\n",
    "    \n",
    "    def _compute_embs(self, repo_codes, batch_size = 1):\n",
    "        repo_embs = []\n",
    "        for repo in repo_codes:\n",
    "            func_names = []\n",
    "            func_contexts = []\n",
    "            for file in repo:\n",
    "                tokens, functions = repo[file]\n",
    "                for func_name, S, T, _ in functions:\n",
    "                    func_names.append(func_name)\n",
    "                    func_contexts.append(' '.join(tokens[S : T]))\n",
    "            code_model, tokenizer = self._update_model_and_tokenizer(self.code_model,\n",
    "                                                                     self.tokenizer,\n",
    "                                                                     func_names,\n",
    "                                                                     func_contexts,\n",
    "                                                                     batch_size)\n",
    "            \n",
    "            \n",
    "        return 0\n",
    "            \n",
    "#         for x in repo:\n",
    "#             print(x, repo[x][1])\n",
    "\n",
    "    def __call__(self, repo_names):\n",
    "        repo_names = np.array(repo_names)\n",
    "        # Step 1: \n",
    "        # Check whether we have the repos in self.keep_code.\n",
    "        # If not, download repos\n",
    "#         download_repo_names = repo_names[~self._if_downloaded(repo_names)]\n",
    "#         download_repos(download_repo_names, self.github_tokens, self.save_path, self.num_process)\n",
    "\n",
    "        # Step 2:\n",
    "        # Convert repo files to tokens\n",
    "        codes = read_repos(repo_names, self.save_path, self.num_process)\n",
    "\n",
    "        # Step 3:\n",
    "        # Use the model to output code feature embs\n",
    "        embs = self._compute_embs(codes)\n",
    "\n",
    "        if self.keep_code == False:\n",
    "#             os.system('rm -r -f ' + self.save_path)\n",
    "            for repo_name in repo_names:\n",
    "                os.system('rm -r -f ' + self.save_path + '/' + repo_name)\n",
    "\n",
    "        return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "cfg = {\n",
    "    'save_path': './repos/',\n",
    "    'keep_code': True,\n",
    "    'github_tokens': [\n",
    "        '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',\n",
    "        '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',\n",
    "        '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',\n",
    "        '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',\n",
    "        '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',\n",
    "        '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',\n",
    "        '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',\n",
    "        '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',\n",
    "    ],\n",
    "    'num_process': 5,\n",
    "    'device': 'cuda',\n",
    "    'model_cfg': {\n",
    "        'hidden_dim': 768,\n",
    "        'code_model': 'microsoft/codebert-base',\n",
    "        'code_trainable': False,\n",
    "        'rnn_trainable': True,\n",
    "    }\n",
    "}\n",
    "worker = CodeFeatureWorker(cfg)\n",
    "# tmp = worker(['AnonymousWorld123/Q-Layer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deep-learning.pk]\n",
      "torch.Size([58, 512])\n",
      "58\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.69 GiB total capacity; 22.16 GiB already allocated; 36.31 MiB free; 22.16 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_84787/3813356360.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#             result.extend(worker(repo_full_names[i * 10 : i * 10 + 10]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_full_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#         save_result(result, '../data', '../data/' + file[ : -3] + '.jsonl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_84787/890288292.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, repo_names)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m# Step 3:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# Use the model to output code feature embs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0membs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_embs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_84787/890288292.py\u001b[0m in \u001b[0;36m_compute_embs\u001b[0;34m(self, repo_codes, batch_size)\u001b[0m\n\u001b[1;32m    119\u001b[0m                     \u001b[0mfunc_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0mfunc_contexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             code_model, tokenizer = self._update_model_and_tokenizer(self.code_model,\n\u001b[0m\u001b[1;32m    122\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                                                                      \u001b[0mfunc_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_84787/890288292.py\u001b[0m in \u001b[0;36m_update_model_and_tokenizer\u001b[0;34m(self, model, tokenizer, func_names, func_contexts, batch_size)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mend_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             func_embs.append(\n\u001b[0;32m---> 90\u001b[0;31m                 model(input_ids = context_tokens['input_ids'][i : end_index],\n\u001b[0m\u001b[1;32m     91\u001b[0m                       attention_mask = context_tokens['attention_mask'][i : end_index])\n\u001b[1;32m     92\u001b[0m                 ['pooler_output']) # 就是这句话爆显存\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         )\n\u001b[0;32m--> 835\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 )\n\u001b[1;32m    521\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    523\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     ):\n\u001b[0;32m--> 337\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    338\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.69 GiB total capacity; 22.16 GiB already allocated; 36.31 MiB free; 22.16 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "def save_result(result, save_dir, save_file):\n",
    "    if os.path.exists(save_dir) == False: os.mkdir(save_dir)\n",
    "    f = open(save_file, 'w')\n",
    "    json.dump(result, fp = f)\n",
    "    f.close()\n",
    "\n",
    "for root, dirs, files in os.walk('../github-topic/'):\n",
    "    for file in files:\n",
    "        if file in ['api.pk']:\n",
    "            continue\n",
    "        if file not in ['android.pk', 'deep-learning.pk']:\n",
    "            continue\n",
    "        print('[' + file + ']')\n",
    "        items = pickle.load(open(root + file, 'rb'))\n",
    "        repo_full_names = [x['full_name'] for x in items]\n",
    "\n",
    "# TODO\n",
    "#         result = []\n",
    "#         for i in tqdm(range(250)):\n",
    "#             result.extend(worker(repo_full_names[i * 10 : i * 10 + 10]))\n",
    "\n",
    "        result = worker(repo_full_names[5 : 6])\n",
    "#         save_result(result, '../data', '../data/' + file[ : -3] + '.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
