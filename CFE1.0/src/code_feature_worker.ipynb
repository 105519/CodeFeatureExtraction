{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "import json\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# import logging\n",
    "# logger = logging.getLogger('root')\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "\n",
    "import time\n",
    "global last_time\n",
    "last_time = time.time()\n",
    "def tprint(s):\n",
    "    return 0\n",
    "    global last_time\n",
    "    print('using time =', round(time.time() - last_time, 2), '----', s)\n",
    "    last_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete(path):\n",
    "    if (os.path.islink(path)):\n",
    "        os.unlink(path)\n",
    "    elif (os.path.isdir(path)):\n",
    "        for x in os.listdir(path):\n",
    "            delete(path + '/' + x)\n",
    "        os.rmdir(path)\n",
    "    else:\n",
    "        assert(os.path.isfile(path))\n",
    "        os.remove(path)\n",
    "\n",
    "def clear(path):\n",
    "    cnt = 0\n",
    "    if (os.path.islink(path)):\n",
    "        delete(path)\n",
    "    elif (os.path.isdir(path)):\n",
    "        for x in os.listdir(path):\n",
    "            if (x[0] == '.'):\n",
    "                delete(path + '/' + x)\n",
    "            else:\n",
    "                cnt += clear(path + '/' + x)\n",
    "        if (cnt == 0):\n",
    "            delete(path)\n",
    "    else:\n",
    "        assert(os.path.isfile(path))\n",
    "        if (path[-3:] == '.py'):\n",
    "            cnt = 1\n",
    "        else:\n",
    "            delete(path)\n",
    "    return cnt\n",
    "\n",
    "def _download_repo(repo_full_name, token, save_path):\n",
    "#     print(f\"git clone https://{token}@github.com.cnpmjs.org/{repo_full_name} {save_path}/{repo_full_name} --progress --depth 1\")\n",
    "    os.system(f\"git clone https://{token}@github.com.cnpmjs.org/{repo_full_name} {save_path}/{repo_full_name} --quiet --depth 1\")\n",
    "#     logger.info(f'Downloaded {repo_full_name}')\n",
    "    clear(save_path + '/' + repo_full_name)\n",
    "\n",
    "def download_repos(repo_full_names, tokens, save_path, num_process = 1):\n",
    "    pool = Pool(num_process)\n",
    "    for repo_full_name in repo_full_names:\n",
    "        token = random.choice(tokens)\n",
    "        pool.apply_async(_download_repo, args = (repo_full_name, token, save_path))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "# def download_repos(repo_full_names, tokens, save_path, num_process = 1):\n",
    "#     for repo_full_name in tqdm(repo_full_names):\n",
    "#         token = random.choice(tokens)\n",
    "#         _download_repo(repo_full_name, token, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_code_tokens_and_functions(path):\n",
    "    code_tokens = []\n",
    "    function_stack = []\n",
    "    code_functions = []\n",
    "    indent_number = 0\n",
    "    with open(path, 'rb') as input_file:\n",
    "        tokenGenerator = tokenize.tokenize(input_file.readline)\n",
    "        for token in tokenGenerator:\n",
    "            if (token.type in [0, 59, 60, 62]): # COMMENT\n",
    "                pass\n",
    "            elif (token.type in [4, 61]): # NEWLINE\n",
    "                pass\n",
    "            elif (token.type == 5): # INDENT\n",
    "                indent_number += 1\n",
    "            elif (token.type == 6): # DEDENT\n",
    "                indent_number -= 1\n",
    "                if (function_stack != [] and indent_number == function_stack[-1][3]):\n",
    "                    code_functions.append(function_stack.pop())\n",
    "                    code_functions[-1][2] = len(code_tokens)\n",
    "            elif (token.type in [1, 2, 3, 54]): # NAME NUMBER STRING OP\n",
    "                code_tokens.append(token.string)\n",
    "                if (token.string in ['def', 'class']):\n",
    "                    function_stack.append([0, len(code_tokens) - 1, 0, indent_number])\n",
    "                elif (function_stack != [] and function_stack[-1][0] == 0):\n",
    "                    function_stack[-1][0] = token.string\n",
    "            else:\n",
    "                assert(False)\n",
    "    return code_tokens, code_functions\n",
    "\n",
    "def _read_codes(repo_path, current_path = '.'):\n",
    "    data = {}\n",
    "    path = os.path.join(repo_path, current_path)\n",
    "    if (os.path.isdir(path)):\n",
    "        for x in os.listdir(path):\n",
    "            data.update(_read_codes(repo_path, current_path + '/' + x))\n",
    "    elif os.path.isfile(path):\n",
    "        if (path[-3 :] != '.py'):\n",
    "            return {}\n",
    "        try:\n",
    "            data[current_path] = _read_code_tokens_and_functions(path)\n",
    "        except:\n",
    "            f = open('badfile.txt', 'a')\n",
    "            print(path, file = f)\n",
    "            f.close()\n",
    "            return {}\n",
    "    else:\n",
    "        return {}\n",
    "    return data\n",
    "\n",
    "def read_repos(repos, filepath, num_process = 1):\n",
    "    dataset = []\n",
    "    pool = Pool(processes = num_process)\n",
    "    for _, repo in enumerate(repos):\n",
    "        dataset.append(pool.apply_async(_read_codes, (filepath + '/' + repo, '.')))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i] = dataset[i].get()\n",
    "    return dataset\n",
    "\n",
    "# def read_repos(repos, filepath, num_process = 1):\n",
    "#     dataset = []\n",
    "#     for _, repo in enumerate(repos):\n",
    "#         dataset.append(_read_codes(filepath + '/' + repo, '.'))\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeFeatureWorker():\n",
    "    def __init__(self, cfg):\n",
    "        self.save_path = cfg['save_path']\n",
    "        if os.path.exists(self.save_path) == False: os.mkdir(self.save_path)\n",
    "\n",
    "        self.keep_code = cfg['keep_code']\n",
    "        self.github_tokens = cfg['github_tokens']\n",
    "        self.num_process = cfg['num_process']\n",
    "        self.device = cfg['device']\n",
    "\n",
    "        self._init_model(cfg['model_cfg'])\n",
    "    \n",
    "    def _init_model(self, cfg):\n",
    "        self.code_trainable = cfg['code_trainable']\n",
    "        self.hidden_dim = cfg['hidden_dim']\n",
    "        self.model_config0 = RobertaConfig.from_pretrained(cfg['code_model'])\n",
    "        self.tokenizer0 = RobertaTokenizer.from_pretrained(cfg['code_model'])\n",
    "        self.code_model0 = RobertaModel.from_pretrained(cfg['code_model'])\n",
    "        \n",
    "        self.model_config = copy.deepcopy(self.model_config0)\n",
    "        self.tokenizer = copy.deepcopy(self.tokenizer0)\n",
    "        self.code_model = copy.deepcopy(self.code_model0).to(self.device)\n",
    "        for param in self.code_model.parameters():\n",
    "            param.requires_grad = self.code_trainable\n",
    "    \n",
    "    def _restart_model(self):\n",
    "        del self.model_config\n",
    "        del self.tokenizer\n",
    "        del self.code_model\n",
    "        self.model_config = copy.deepcopy(self.model_config0)\n",
    "        self.tokenizer = copy.deepcopy(self.tokenizer0)\n",
    "        self.code_model = copy.deepcopy(self.code_model0).to(self.device)\n",
    "        for param in self.code_model.parameters():\n",
    "            param.requires_grad = self.code_trainable\n",
    "\n",
    "    def _if_downloaded(self, repo_names):\n",
    "        return np.array([\n",
    "            os.path.exists(\n",
    "                os.path.join(self.save_path, repo_name)\n",
    "            ) for repo_name in repo_names\n",
    "        ])\n",
    "    \n",
    "    def _embed_functions(self, context_tokens, batch_size = 32):\n",
    "        func_embs = []\n",
    "        total_size = len(context_tokens['input_ids'])\n",
    "        for i in range(0, total_size, batch_size):\n",
    "            end_index = min(total_size, i + batch_size)\n",
    "            with torch.no_grad():\n",
    "                func_embs.append(\n",
    "                    self.code_model(input_ids = context_tokens['input_ids'][i : end_index],\n",
    "                                    attention_mask = context_tokens['attention_mask'][i : end_index])\n",
    "                    ['pooler_output'])\n",
    "        func_embs = torch.cat(func_embs, dim = 0)\n",
    "        return func_embs\n",
    "    \n",
    "    def _compute_embs(self, repo_codes):\n",
    "        repo_embs = []\n",
    "        for repo_code in tqdm(repo_codes):\n",
    "            func_names = []\n",
    "            func_contexts = []\n",
    "            for file in repo_code:\n",
    "                tokens, functions = repo_code[file]\n",
    "                for func_name, S, T, _ in functions:\n",
    "                    func_names.append(func_name)\n",
    "                    func_contexts.append(' '.join(tokens[S : min(T, S + 512)]))\n",
    "            if (len(func_names) == 0 or len(func_names) > 1600):\n",
    "                repo_embs.append(-1)\n",
    "                continue\n",
    "            tprint('s3.0 funcs got : %d' % len(func_names))\n",
    "            \n",
    "            self._restart_model()\n",
    "            tprint('s3.05 restart done')\n",
    "\n",
    "            # update tokenizer\n",
    "            _ = self.tokenizer.add_tokens(func_names)\n",
    "            self.code_model.resize_token_embeddings(len(self.tokenizer))\n",
    "            tprint('s3.1 tokenize')\n",
    "            context_tokens = self.tokenizer(func_contexts, return_tensors=\"pt\",\n",
    "                                            truncation=True, padding='max_length').to(self.device)\n",
    "            tprint('s3.1 tokenize done')\n",
    "            func_embs = self._embed_functions(context_tokens)\n",
    "            func_token_ids = self.tokenizer.convert_tokens_to_ids(func_names)\n",
    "            with torch.no_grad():\n",
    "                self.code_model.embeddings.word_embeddings.weight[func_token_ids] = func_embs\n",
    "            tprint('s3.3 first run & weighted')\n",
    "            \n",
    "            # compute embedding\n",
    "            repo_embs.append(self._embed_functions(context_tokens).mean(0).to('cpu').tolist())\n",
    "            tprint('s3.4 second run\\n')\n",
    "            \n",
    "#             # delete tokens\n",
    "#             self.tokenizer.added_tokens_encoder.clear()\n",
    "#             self.code_model.resize_token_embeddings(len(self.tokenizer))\n",
    "            \n",
    "        return repo_embs\n",
    "\n",
    "    def __call__(self, repo_names):\n",
    "        repo_names = np.array(repo_names)\n",
    "        # Step 1: \n",
    "        # Check whether we have the repos in self.keep_code.\n",
    "        # If not, download repos\n",
    "#         tprint('step1')\n",
    "#         download_repo_names = repo_names[~self._if_downloaded(repo_names)]\n",
    "#         download_repos(download_repo_names, self.github_tokens, self.save_path, self.num_process)\n",
    "\n",
    "        # Step 2:\n",
    "        # Convert repo files to tokens\n",
    "        tprint('step2')\n",
    "        codes = read_repos(repo_names, self.save_path, self.num_process)\n",
    "\n",
    "        # Step 3:\n",
    "        # Use the model to output code feature embs\n",
    "        tprint('step3')\n",
    "        repo_embs = self._compute_embs(codes)\n",
    "        \n",
    "        if self.keep_code == False:\n",
    "            for repo_name in repo_names:\n",
    "                os.system('rm -r -f ' + self.save_path + '/' + repo_name)\n",
    "            clear(self.save_path)\n",
    "\n",
    "        return [[x, y] for x, y in zip(repo_names, repo_embs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "cfg = {\n",
    "    'save_path': './repos/',\n",
    "    'keep_code': True,\n",
    "    'github_tokens': [\n",
    "        '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',\n",
    "        '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',\n",
    "        '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',\n",
    "        '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',\n",
    "        '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',\n",
    "        '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',\n",
    "        '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',\n",
    "        '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',\n",
    "    ],\n",
    "    'num_process': 10,\n",
    "    'device': 'cuda',\n",
    "    'model_cfg': {\n",
    "        'hidden_dim': 768,\n",
    "        'code_model': 'microsoft/codebert-base',\n",
    "        'code_trainable': False,\n",
    "    }\n",
    "}\n",
    "worker = CodeFeatureWorker(cfg)\n",
    "# result = worker(['AnonymousWorld123/Q-Layer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deep-learning.pk]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2500/2500 [1:28:04<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[android.pk]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2500/2500 [1:24:01<00:00,  2.02s/it]\n"
     ]
    }
   ],
   "source": [
    "def save_result(result, save_dir, save_file):\n",
    "    if os.path.exists(save_dir) == False: os.mkdir(save_dir)\n",
    "    f = open(save_file, 'w')\n",
    "    json.dump(result, fp = f)\n",
    "    f.close()\n",
    "\n",
    "for root, dirs, files in os.walk('../github-topic/'):\n",
    "    for file in files:\n",
    "        if file in ['api.pk']:\n",
    "            continue\n",
    "        if file not in ['android.pk', 'deep-learning.pk']:\n",
    "            continue\n",
    "        print('[' + file + ']')\n",
    "        items = pickle.load(open(root + file, 'rb'))\n",
    "        repo_full_names = [x['full_name'] for x in items]\n",
    "\n",
    "#         result = []\n",
    "#         for i in tqdm(range(250)):\n",
    "#             result.extend(worker(repo_full_names[i * 10 : i * 10 + 10]))\n",
    "\n",
    "        result = worker(repo_full_names[0 : 2500])\n",
    "#         result = worker(repo_full_names[5 : 6])\n",
    "\n",
    "        save_result(result, '../data', '../data/' + file[ : -3] + '.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
