{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizer\n",
    "# from transformers import WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig.from_pretrained('microsoft/graphcodebert-base')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/graphcodebert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 137711/137711 [02:02<00:00, 1125.21it/s]\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename) as f:\n",
    "        text = []\n",
    "        for line in f:\n",
    "            text.append(line.strip())\n",
    "        bar = tqdm(text, total=len(text))\n",
    "        examples = []\n",
    "        for x in bar:\n",
    "            examples.append(eval(x))\n",
    "    return examples\n",
    "\n",
    "dataset = read_data('../py150_files/washed_python150k.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = 512\n",
    "graph_length = 200\n",
    "\n",
    "epochs = 10\n",
    "train_batch_size = 16\n",
    "eval_batch_size = 16\n",
    "\n",
    "seed = 978\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "learning_rate = 5e-5\n",
    "max_steps = -1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    def __init__(self, code_ids, position_idx, edges, cross_edges):\n",
    "        self.code_ids = code_ids\n",
    "        self.position_idx = position_idx\n",
    "        self.edges = edges\n",
    "        self.cross_edges = cross_edges\n",
    "\n",
    "def convert_example_to_feature(example):\n",
    "    tokens = example['tokens']\n",
    "    nodes = example['nodes']\n",
    "    edges = example['edges']\n",
    "    cross_edges = example['cross_edges']\n",
    "\n",
    "    code_length = total_length - min(graph_length, len(nodes)) - 3\n",
    "    tokens = tokens[: code_length] \n",
    "    tokens = [tokenizer.tokenize(tokens[0])] \\\n",
    "           + [tokenizer.tokenize('@ ' + x)[1 :] for x in tokens[1 :]]\n",
    "    ori2cur_pos = {-1 : (0, 0)}\n",
    "    for i in range(len(tokens)):\n",
    "        ori2cur_pos[i] = (ori2cur_pos[i - 1][1], ori2cur_pos[i - 1][1] + len(tokens[i]))\n",
    "    tokens=[y for x in tokens for y in x] \n",
    "\n",
    "    #truncating\n",
    "    tokens = tokens[: code_length]\n",
    "    nodes = nodes[: graph_length]\n",
    "    edges = [(a, b) for (a, b) in edges if (a < len(nodes)) and (b < len(nodes))]\n",
    "    cross_edges = [(ori2cur_pos[a], b) for (a, b) in cross_edges\\\n",
    "                   if (a in ori2cur_pos) and (ori2cur_pos[a][1] < len(tokens)) and (b < len(nodes))]\n",
    "\n",
    "    #adding code tokens\n",
    "    code_tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
    "    code_ids = tokenizer.convert_tokens_to_ids(code_tokens)\n",
    "    position_idx = [i + tokenizer.pad_token_id + 1 for i in range(len(code_tokens))]\n",
    "\n",
    "    #adding graph nodes\n",
    "    code_tokens += [x for x in nodes]\n",
    "    code_ids += [tokenizer.unk_token_id] * len(nodes)\n",
    "    position_idx += [0] * len(nodes)\n",
    "    assert(len(code_ids) == len(position_idx))\n",
    "    assert(len(code_ids) < total_length)\n",
    "\n",
    "    #padding\n",
    "    padding_length = total_length - len(code_ids)\n",
    "    code_ids += [tokenizer.pad_token_id] * padding_length\n",
    "    position_idx += [tokenizer.pad_token_id] * padding_length\n",
    "    return InputFeatures(code_ids, position_idx, edges, cross_edges)\n",
    "\n",
    "def convert_examples_to_features(examples):\n",
    "    features = []\n",
    "    pool = multiprocessing.Pool(processes = 24)\n",
    "    for example in examples:\n",
    "        features.append(pool.apply_async(convert_example_to_feature, (example, )))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    for i in range(len(features)):\n",
    "        features[i] = features[i].get()\n",
    "    return features\n",
    "\n",
    "dataset2 = dataset\n",
    "# random.shuffle(dataset2)\n",
    "train_examples = dataset2[: int(len(dataset2) * 0.67)]\n",
    "eval_examples = dataset2[int(len(dataset2) * 0.67) :]\n",
    "train_features = convert_examples_to_features(train_examples)\n",
    "eval_features = convert_examples_to_features(eval_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class InputFeatures(object):\n",
    "#     def __init__(self, code_ids, position_idx, edges, cross_edges):\n",
    "#         self.code_ids = code_ids\n",
    "#         self.position_idx = position_idx\n",
    "#         self.edges = edges\n",
    "#         self.cross_edges = cross_edges\n",
    "\n",
    "# def convert_examples_to_features(examples):\n",
    "#     features = []\n",
    "#     for example in tqdm(examples, total = len(examples)):\n",
    "#         tokens = example['tokens']\n",
    "#         nodes = example['nodes']\n",
    "#         edges = example['edges']\n",
    "#         cross_edges = example['cross_edges']\n",
    "        \n",
    "#         code_length = total_length - min(graph_length, len(nodes)) - 3\n",
    "#         tokens = tokens[: code_length] \n",
    "#         tokens = [tokenizer.tokenize(tokens[0])] \\\n",
    "#                + [tokenizer.tokenize('@ ' + x)[1 :] for x in tokens[1 :]]\n",
    "#         ori2cur_pos = {-1 : (0, 0)}\n",
    "#         for i in range(len(tokens)):\n",
    "#             ori2cur_pos[i] = (ori2cur_pos[i - 1][1], ori2cur_pos[i - 1][1] + len(tokens[i]))\n",
    "#         tokens=[y for x in tokens for y in x] \n",
    "        \n",
    "#         #truncating\n",
    "#         tokens = tokens[: code_length]\n",
    "#         nodes = nodes[: graph_length]\n",
    "#         edges = [(a, b) for (a, b) in edges if (a < len(nodes)) and (b < len(nodes))]\n",
    "#         cross_edges = [(ori2cur_pos[a], b) for (a, b) in cross_edges\\\n",
    "#                        if (a in ori2cur_pos) and (ori2cur_pos[a][1] < len(tokens)) and (b < len(nodes))]\n",
    "        \n",
    "#         #adding code tokens\n",
    "#         code_tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
    "#         code_ids = tokenizer.convert_tokens_to_ids(code_tokens)\n",
    "#         position_idx = [i + tokenizer.pad_token_id + 1 for i in range(len(code_tokens))]\n",
    "        \n",
    "#         #adding graph nodes\n",
    "#         code_tokens += [x for x in nodes]\n",
    "#         code_ids += [tokenizer.unk_token_id] * len(nodes)\n",
    "#         position_idx += [0] * len(nodes)\n",
    "#         assert(len(code_ids) == len(position_idx))\n",
    "#         assert(len(code_ids) < total_length)\n",
    "        \n",
    "#         #padding\n",
    "#         padding_length = total_length - len(code_ids)\n",
    "#         code_ids += [tokenizer.pad_token_id] * padding_length\n",
    "#         position_idx += [tokenizer.pad_token_id] * padding_length\n",
    "#         features.append(InputFeatures(code_ids, position_idx, edges, cross_edges))\n",
    "#     return features\n",
    "\n",
    "# dataset2 = dataset\n",
    "# # random.shuffle(dataset2)\n",
    "# train_examples = dataset2[: int(len(dataset) * 0.67)]\n",
    "# eval_examples = dataset2[int(len(dataset) * 0.67) :]\n",
    "# train_features = convert_examples_to_features(train_examples)\n",
    "# eval_features = convert_examples_to_features(eval_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        attn_mask = np.zeros((total_length, total_length), dtype = np.bool)\n",
    "        node_index = sum([i > 1 for i in self.examples[item].position_idx])\n",
    "        max_length = sum([i != 1 for i in self.examples[item].position_idx])\n",
    "        \n",
    "        attn_mask[: node_index, : node_index] = True\n",
    "        for i, x in enumerate(self.examples[item].code_ids):\n",
    "            if x in [tokenizer.cls_token_id, tokenizer.sep_token_id]:\n",
    "                attn_mask[i, 0 : max_length] = True # [cls/sep, all]\n",
    "                attn_mask[0 : max_length, i] = True # test [all, cls/sep]\n",
    "        attn_mask[1 : node_index - 1, node_index] = True # cross edge (token, graph ROOT)\n",
    "        attn_mask[node_index, 1 : node_index - 1] = True # cross edge (graph ROOT, token)\n",
    "        for ((a, b), c) in self.examples[item].cross_edges:\n",
    "            attn_mask[a + 1 : b + 1, node_index + c] = True # cross edge (token, graph node)\n",
    "            attn_mask[node_index + c, a + 1 : b + 1] = True # cross edge (token, graph node)\n",
    "        for (a, b) in self.examples[item].edges:\n",
    "            attn_mask[node_index + a, node_index + b] = True # edge (source, target)\n",
    "#             attn_mask[node_index + b, node_index + a] = True # test\n",
    "\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "        for x in self.examples[item].code_ids:\n",
    "            if (x in [tokenizer.cls_token_id, tokenizer.sep_token_id,\n",
    "                      tokenizer.unk_token_id, tokenizer.pad_token_id]):\n",
    "                input_ids.append(x)\n",
    "                labels.append(-100)\n",
    "            elif (random.randint(0, 99) < 15):\n",
    "                input_ids.append(tokenizer.mask_token_id)\n",
    "                labels.append(x)\n",
    "            else:\n",
    "                input_ids.append(x)\n",
    "                labels.append(-100)\n",
    "\n",
    "        return (torch.tensor(input_ids),\n",
    "                torch.tensor(self.examples[item].position_idx),\n",
    "                torch.tensor(attn_mask),\n",
    "                torch.tensor(labels))\n",
    "\n",
    "train_data = TextDataset(train_features)\n",
    "eval_data = TextDataset(eval_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_data)\n",
    "eval_sampler = RandomSampler(eval_data)\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, drop_last = True,\n",
    "                              batch_size = train_batch_size, num_workers = 4)\n",
    "eval_dataloader = DataLoader(eval_data, sampler = eval_sampler, shuffle = False, drop_last = False,\n",
    "                             batch_size = eval_batch_size, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained('microsoft/graphcodebert-base', config = config)\n",
    "model.to(device)\n",
    "\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# no_decay = ['bias', 'LayerNorm.weight']\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#      'weight_decay': weight_decay},\n",
    "#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "# ]\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr = learning_rate, eps = adam_epsilon)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warmup_steps,\n",
    "#                                             num_training_steps = max_steps)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "avg_acc = 0\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: loss 0.43 acc 0 best 0: 100%|████████████| 5766/5766 [36:19<00:00,  2.65it/s]\n",
      "100%|███████████████████████████████████████| 2841/2841 [06:05<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9135488409191111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1: loss 0.39 acc 91.35 best 91.35: 100%|████| 5766/5766 [36:42<00:00,  2.62it/s]\n",
      "100%|███████████████████████████████████████| 2841/2841 [06:07<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9138852969566722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2: loss 0.36 acc 91.39 best 91.39:  29%|█▏  | 1654/5766 [10:40<26:31,  2.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3979572/1524317555.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_id in range(epochs): \n",
    "    train_num = 0\n",
    "    train_loss = 0\n",
    "    avg_loss = 0\n",
    "    bar = tqdm(train_dataloader, total = len(train_dataloader))\n",
    "    bar.set_description(\"{}: loss {} acc {} best {}\".\\\n",
    "                        format(epoch_id, round(avg_loss, 2), round(avg_acc * 100, 2), round(best_acc * 100, 2)))\n",
    "\n",
    "    for step, batch in enumerate(bar):\n",
    "        (input_ids, position_ids, attention_mask, labels) = [x.to(device) for x in batch]\n",
    "        output = model(input_ids = input_ids,\n",
    "                       position_ids = position_ids,\n",
    "                       attention_mask = attention_mask,\n",
    "                       labels = labels)\n",
    "        loss = output.loss\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        train_num += 1\n",
    "        train_loss += loss.item()\n",
    "        avg_loss = train_loss / train_num\n",
    "        bar.set_description(\"{}: loss {} acc {} best {}\".\\\n",
    "                            format(epoch_id, round(avg_loss, 2), round(avg_acc * 100, 2), round(best_acc * 100, 2)))\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "#             scheduler.step()\n",
    "\n",
    "    if ((epoch_id + 1) % 1 == 0):\n",
    "        bar = tqdm(eval_dataloader, total = len(eval_dataloader))\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for batch in bar:\n",
    "            (input_ids, position_ids, attention_mask, labels) = [x.to(device) for x in batch]\n",
    "            with torch.no_grad():\n",
    "                output = model(input_ids = input_ids,\n",
    "                               position_ids = position_ids,\n",
    "                               attention_mask = attention_mask)\n",
    "            _, predicted = torch.max(output.logits, 2)\n",
    "            predicted = predicted.view(1, -1).squeeze()\n",
    "            labels = labels.view(1, -1).squeeze()\n",
    "            total += (labels != -100).sum().item()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        avg_acc = correct / total\n",
    "        best_acc = max(best_acc, avg_acc)\n",
    "        print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
