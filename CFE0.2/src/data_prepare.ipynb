{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import tokenize\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from parser import remove_comments_and_docstrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_list = []\n",
    "for dir1 in os.listdir('../py150_files/data/'):\n",
    "    assert(os.path.isdir('../py150_files/data/' + dir1))\n",
    "    for dir2 in os.listdir('../py150_files/data/' + dir1):\n",
    "        assert(os.path.isdir('../py150_files/data/' + dir1 + '/' + dir2))\n",
    "        project_list.append(dir1 + '/' + dir2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141349\n"
     ]
    }
   ],
   "source": [
    "def get_list(data_list, filename):\n",
    "    file = open('../py150_files/' + filename + '.txt')\n",
    "    context = file.read()\n",
    "    file.close()\n",
    "    file_list = context.strip().split('\\n')\n",
    "    \n",
    "    for x in file_list:\n",
    "        project = x[5 : x.find('/', x.find('/', 5) + 1)]\n",
    "        assert(os.path.isdir('../py150_files/data/' + project))\n",
    "        assert(os.path.isfile('../py150_files/' + x))\n",
    "        if (os.path.isdir('../py150_files/structure/' + project)):\n",
    "            if (os.path.isfile('../py150_files/structure/' + project + '/node.txt')):\n",
    "                if (os.path.isfile('../py150_files/structure/' + project + '/edge.txt')):\n",
    "                    data_list.append(x[5 :])\n",
    "\n",
    "data_list = []\n",
    "get_list(data_list, 'python50k_eval')\n",
    "get_list(data_list, 'python100k_train')\n",
    "data_list.sort()\n",
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(path):\n",
    "    nodes = {}\n",
    "    edges = []\n",
    "    root = -1\n",
    "    \n",
    "    file = open(path + '/node.txt')\n",
    "    context = file.read()\n",
    "    context = context.strip().split('\\n')\n",
    "    file.close()\n",
    "    for x in context[1 :]:\n",
    "        node_id = int(x[0 : x.find(',')])\n",
    "        node_type = x[x.find(',') + 1 : x.find(',', x.find(',') + 1)]\n",
    "        node_name = x[x.find(',', x.find(',') + 1) + 1 : x.rfind(',')]\n",
    "        node_father = int(x[x.rfind(',') + 1 :])\n",
    "        assert(node_id not in nodes)\n",
    "        nodes[node_id] = (node_type, node_name, node_father)\n",
    "    \n",
    "    file = open(path + '/edge.txt')\n",
    "    context = file.read()\n",
    "    context = context.strip().split('\\n')\n",
    "    file.close()\n",
    "    for x in context[1 :]:\n",
    "        source = int(x[: x.find(',')])\n",
    "        target = int(x[x.find(',') + 1 : x.find(',', x.find(',') + 1)])\n",
    "        if ((source in nodes) and (target in nodes)):\n",
    "            edges.append((source, target))\n",
    "    for node_id in nodes:\n",
    "        node_father = nodes[node_id][2]\n",
    "        if (node_father in nodes):\n",
    "            edges.append((node_father, node_id))\n",
    "        else:\n",
    "            assert(node_father == -1)\n",
    "\n",
    "    E = {}\n",
    "    for x in nodes:\n",
    "        E[x] = []\n",
    "    for (u, v) in edges:\n",
    "        E[u].append(v)\n",
    "        E[v].append(u)\n",
    "    return nodes, edges, E\n",
    "\n",
    "def find_root(nodes, root_name):\n",
    "    root = -1\n",
    "    for x in nodes:\n",
    "        if (nodes[x][1][-len(root_name) :] == root_name):\n",
    "            root = x\n",
    "    return root\n",
    "\n",
    "def cut_graph(nodes, edges, root, E):\n",
    "    visited = set()\n",
    "    visited.add(root)\n",
    "    Q1 = [root]\n",
    "    remain_nodes = [root]\n",
    "    while (len(remain_nodes) < 200):\n",
    "        Q2 = []\n",
    "        for u in Q1:\n",
    "            for v in E[u]:\n",
    "                if (v not in visited):\n",
    "                    visited.add(v)\n",
    "                    Q2.append(v)\n",
    "        if (len(Q2) == 0):\n",
    "            break\n",
    "        Q1 = Q2\n",
    "        random.shuffle(Q1)\n",
    "        for u in Q1:\n",
    "            assert(u not in remain_nodes)\n",
    "            if (u not in remain_nodes):\n",
    "                node_type = nodes[u][0]\n",
    "                node_father = nodes[u][2]\n",
    "                if (node_father in nodes):\n",
    "                    father_type = nodes[node_father][0]\n",
    "                else:\n",
    "                    father_type = 'File'\n",
    "                if ((node_type != 'Variable') and (father_type in ['Package', 'File'])):\n",
    "                    remain_nodes.append(u)\n",
    "    remain_nodes = remain_nodes[: 200]\n",
    "    \n",
    "    new_nodes = []\n",
    "    new_edges = []\n",
    "    idmap = {}\n",
    "    for i, x in enumerate(remain_nodes):\n",
    "        new_nodes.append(nodes[x][1])\n",
    "        idmap[x] = i\n",
    "    for (u, v) in edges:\n",
    "        if ((u in idmap) and (v in idmap)):\n",
    "            new_edges.append((idmap[u], idmap[v]))\n",
    "    return new_nodes, new_edges\n",
    "\n",
    "def get_tokens(path, nodes, do_remove, idx):\n",
    "    input_file = open(path)\n",
    "    code = input_file.read()\n",
    "    input_file.close()\n",
    "    if (do_remove):\n",
    "        code = remove_comments_and_docstrings(code, 'python')\n",
    "    output_file = open('tmp' + str(idx) + '.py', 'w')\n",
    "    print(code, file = output_file)\n",
    "    output_file.close()\n",
    "    \n",
    "    tokens = []\n",
    "    cross_edges = []\n",
    "    f = open('tmp' + str(idx) + '.py', 'rb')\n",
    "    tokenGenerator = tokenize.tokenize(f.readline)\n",
    "    for token in tokenGenerator:\n",
    "        if (token.type in [0, 59, 60, 62]): # COMMENT\n",
    "            pass\n",
    "        elif (token.type in [4, 61]): # NEWLINE\n",
    "            pass\n",
    "        elif (token.type == 5): # INDENT\n",
    "            tokens.append('{')\n",
    "            pass\n",
    "        elif (token.type == 6): # DEDENT\n",
    "            tokens.append('}')\n",
    "            pass\n",
    "        elif (token.type in [1, 2, 3, 54]): # NAME NUMBER STRING OP\n",
    "            for i in range(len(nodes)):\n",
    "                if (token.string == nodes[i]):\n",
    "                    cross_edges.append((len(tokens), i))\n",
    "            tokens.append(token.string)\n",
    "        else:\n",
    "            assert(False)\n",
    "    f.close()\n",
    "    return tokens, cross_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-thread\n",
    "\n",
    "# output_file = open('../py150_files/washed_python150k.txt', 'w')\n",
    "# last_project = ''\n",
    "# for x in tqdm(data_list, total = len(data_list)):\n",
    "#     project = x[: x.find('/', x.find('/') + 1)]\n",
    "#     if (project == last_project):\n",
    "#         nodes, edges, E = last_nodes, last_edges, last_E\n",
    "#     else:\n",
    "#         last_project = project\n",
    "#         nodes, edges, E = get_graph('../py150_files/structure/' + project)\n",
    "#         last_nodes, last_edges, last_E = nodes, edges, E\n",
    "#     root = find_root(nodes, x)\n",
    "#     if (root == -1):\n",
    "#         continue\n",
    "#     nodes, edges = cut_graph(nodes, edges, root, E)\n",
    "#     try:\n",
    "#         tokens, cross_edges = get_tokens('../py150_files/data/' + x, nodes, True, 0)\n",
    "#     except:\n",
    "#         tokens, cross_edges = get_tokens('../py150_files/data/' + x, nodes, False, 0)\n",
    "#     os.system('rm tmp.py')\n",
    "#     if (len(tokens) != 0):\n",
    "#         data = {}\n",
    "#         data['path'] = x\n",
    "#         data['tokens'] = tokens\n",
    "#         data['nodes'] = nodes\n",
    "#         data['edges'] = edges\n",
    "#         data['cross_edges'] = cross_edges\n",
    "#         print(data, file = output_file)\n",
    "# output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using time = 598.32\n"
     ]
    }
   ],
   "source": [
    "# muti-thread\n",
    "\n",
    "def work(path, idx):\n",
    "    project = path[: path.find('/', path.find('/') + 1)]\n",
    "    nodes, edges, E = get_graph('../py150_files/structure/' + project)\n",
    "    root = find_root(nodes, path)\n",
    "    if (root == -1):\n",
    "        return 0\n",
    "    nodes, edges = cut_graph(nodes, edges, root, E)\n",
    "    try:\n",
    "        tokens, cross_edges = get_tokens('../py150_files/data/' + path, nodes, True, idx)\n",
    "    except:\n",
    "        tokens, cross_edges = get_tokens('../py150_files/data/' + path, nodes, False, idx)\n",
    "    os.system('rm tmp' + str(idx) + '.py')\n",
    "    if (len(tokens) != 0):\n",
    "        data = {}\n",
    "        data['path'] = path\n",
    "        data['tokens'] = tokens\n",
    "        data['nodes'] = nodes\n",
    "        data['edges'] = edges\n",
    "        data['cross_edges'] = cross_edges\n",
    "        return data\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "start_time = time.time()\n",
    "dataset = []\n",
    "pool = multiprocessing.Pool(processes = 26)\n",
    "for idx, path in enumerate(data_list):\n",
    "    dataset.append(pool.apply_async(work, (path, idx, )))\n",
    "pool.close()\n",
    "pool.join()\n",
    "print('using time =', round(time.time() - start_time, 2))\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].get()\n",
    "\n",
    "output_file = open('../py150_files/washed_python150k.txt2', 'w')\n",
    "for data in dataset:\n",
    "    if (data != 0):\n",
    "        print(data, file = output_file)\n",
    "output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
