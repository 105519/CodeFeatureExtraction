{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "# os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizer\n",
    "from transformers import WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
    "# from parser import remove_comments_and_docstrings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# sentence_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/dataset.jsonl', 'r')\n",
    "dataset0 = json.loads(f.readline())\n",
    "f.close()\n",
    "f = open('../data/graphs2.jsonl', 'r')\n",
    "graphs2 = json.loads(f.readline())\n",
    "f.close()\n",
    "f = open('../data/labels01.jsonl', 'r')\n",
    "labels01 = json.loads(f.readline())\n",
    "f.close()\n",
    "topic_number = len(labels01[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arguments(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.epochs = 25\n",
    "args.input_limit = 10\n",
    "args.gradient_accumulation_steps = 32\n",
    "\n",
    "args.total_length = 512\n",
    "args.graph_length = 200\n",
    "args.max_grad_norm = 1.0\n",
    "args.learning_rate = 1e-5\n",
    "args.weight_decay = 0.0\n",
    "args.adam_epsilon = 1e-8\n",
    "\n",
    "args.current_topic = 0\n",
    "args.topic_number = topic_number\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.seed = 978438233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "set_seed()\n",
    "\n",
    "config = RobertaConfig.from_pretrained('microsoft/codebert-base')\n",
    "config.num_labels = 1\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "pretrain_model = RobertaForMaskedLM.from_pretrained('microsoft/codebert-base', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for x, E, y in tqdm(zip(dataset0, graphs2, labels01), total = len(graphs2)):\n",
    "    if (len(x) != 0 and sum(y) != 0):\n",
    "        V = [[1] + [0 for i in range(384 - 1)] for j in range(200)]\n",
    "        dataset.append([x, V, E, y])\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# dataset = []\n",
    "# for x, (V, E), y in tqdm(zip(dataset0, graphs2, labels01), total = len(graphs2)):\n",
    "#     if (len(x) != 0 and sum(y) != 0):\n",
    "#         for i in range(200 - len(V)):\n",
    "#             V.append([0] * 384)\n",
    "#         dataset.append([x, V, E, y])\n",
    "# random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        code, V, M, labels = self.examples[item]\n",
    "        random.shuffle(code)\n",
    "        \n",
    "        code_ids = []\n",
    "        position_ids = []\n",
    "        for x in code:\n",
    "            code_ids.append([y for y in x if y != 1] + [1])\n",
    "            position_ids.append([i + tokenizer.pad_token_id + 1 for i in range(len(code_ids[-1]) - 1)] + [1])\n",
    "        code_ids = [y for x in code_ids for y in x]\n",
    "        position_ids = [y for x in position_ids for y in x]\n",
    "        \n",
    "        length = args.input_limit * args.total_length \n",
    "        code_ids = code_ids[: length]\n",
    "        position_ids = position_ids[: length]\n",
    "        code_ids.extend([1] * (length - len(code_ids)))\n",
    "        position_ids.extend([1] * (length - len(position_ids)))\n",
    "        \n",
    "        code_ids = torch.tensor(code_ids).view(-1, args.total_length)\n",
    "        position_ids = torch.tensor(position_ids).view(-1, args.total_length)\n",
    "        return code_ids, position_ids, torch.tensor(V), M, torch.Tensor([labels[args.current_topic]])\n",
    "\n",
    "test_data = TextDataset(dataset[int(len(dataset) * 0.75) :])\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, drop_last = False)#, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.W = nn.Linear(384 * 2, 384)\n",
    "        self.encoder = encoder\n",
    "        self.config = config\n",
    "        self.rnn = nn.LSTM(config.hidden_size, config.hidden_size, 1)\n",
    "        self.first_dense = nn.Linear(384, 40)\n",
    "        \n",
    "        self.dense = nn.Linear(config.hidden_size + args.graph_length * 40, config.hidden_size * 6)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out_proj = nn.Linear(config.hidden_size * 6, 2)\n",
    "        \n",
    "    def forward(self, code_ids, position_ids, nodes, edges, labels):\n",
    "        nodes = nodes.view(args.graph_length, -1)\n",
    "        for k in range(20):\n",
    "            new_nodes = []\n",
    "            for u in range(nodes.size(0)):\n",
    "                h = torch.zeros(384).to(args.device)\n",
    "                V = random.sample(edges[u], min(5, len(edges[u])))\n",
    "                for v in V:\n",
    "                    h += nodes[v] / len(V)\n",
    "                h = torch.cat((nodes[u], h))\n",
    "                h = F.relu(self.W(h))\n",
    "                new_nodes.append(h / (h * h).sum())\n",
    "            nodes = torch.stack(new_nodes, dim = 0)\n",
    "        y = F.relu(self.first_dense(nodes)).view(-1)\n",
    "\n",
    "        h = torch.randn(1, code_ids.size(0), self.config.hidden_size).to(args.device)\n",
    "        c = torch.randn(1, code_ids.size(0), self.config.hidden_size).to(args.device)\n",
    "        code_ids = code_ids.transpose(0, 1)\n",
    "        position_ids = position_ids.transpose(0, 1)\n",
    "        code_embeddings = self.encoder.roberta.embeddings.word_embeddings(code_ids)\n",
    "        for i in range(code_embeddings.size(0)):\n",
    "            bert_output = self.encoder.roberta(inputs_embeds = code_embeddings[i],\n",
    "                                               position_ids = position_ids[i])\n",
    "            _, (h, c) = self.rnn(bert_output[0][:, 0, :].view(1, -1, config.hidden_size), (h, c))\n",
    "        x = h[0].view(-1)\n",
    "        \n",
    "        x = torch.cat((x, y), dim = 0)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        x = F.softmax(x, dim = 0)[1:]\n",
    "        loss_function = MSELoss()\n",
    "        loss = loss_function(x.view(-1), labels.view(-1))\n",
    "        return x, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, epoch_id):\n",
    "    loss_sum = 0\n",
    "    loss_cnt = 0\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    bar = tqdm(test_dataloader, total = len(test_dataloader))\n",
    "    for data in bar:\n",
    "        code_ids, position_ids, nodes, edges, labels = data\n",
    "        code_ids = code_ids.to(args.device)\n",
    "        position_ids = position_ids.to(args.device)\n",
    "        nodes = nodes.to(args.device)\n",
    "        edges = [[b.item() for b in a] for a in edges]\n",
    "        labels = labels.to(args.device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prob, loss = model(code_ids, position_ids, nodes, edges, labels)\n",
    "            prob = prob.view(-1)\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "            loss_sum = loss_sum + loss.item() * code_ids.size(0)\n",
    "            loss_cnt = loss_cnt + code_ids.size(0)\n",
    "            y_preds.append((prob > 0.5).long().cpu().numpy())\n",
    "            y_trues.append(labels.long().view(-1).cpu().numpy())\n",
    "    y_trues = np.concatenate(y_trues, 0)\n",
    "    y_preds = np.concatenate(y_preds, 0)\n",
    "    TP = sum([x == 1 and y == 1 for x, y in zip(y_trues, y_preds)])\n",
    "    FP = sum([x == 0 and y == 1 for x, y in zip(y_trues, y_preds)])\n",
    "    TN = sum([x == 0 and y == 0 for x, y in zip(y_trues, y_preds)])\n",
    "    FN = sum([x == 1 and y == 0 for x, y in zip(y_trues, y_preds)])\n",
    "    print('TP FP TN FN =', TP, FP, TN, FN)\n",
    "\n",
    "    f1 = float(f1_score(y_trues, y_preds))\n",
    "    rs = float(recall_score(y_trues, y_preds))\n",
    "    ps = float(precision_score(y_trues, y_preds))\n",
    "    os.system('mkdir -p result')\n",
    "    print('f1:', f1)\n",
    "    print('recall:', rs)\n",
    "    print('precision:', ps)\n",
    "    print('loss:', loss_sum / loss_cnt)\n",
    "    f = open('result/concat' + str(args.current_topic).zfill(2) + '-' + str(epoch_id).zfill(3) + '.txt', 'w')\n",
    "    print(f1, rs, ps, loss_sum / loss_cnt, TP, FP, TN, FN, file = f)\n",
    "    f.close()\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_dataloader():\n",
    "    posi_data = []\n",
    "    nega_data = []\n",
    "    for x in dataset[: int(len(dataset) * 0.75)]:\n",
    "        if (x[3][args.current_topic]):\n",
    "            posi_data.append(x)\n",
    "        else:\n",
    "            nega_data.append(x)\n",
    "    print(len(posi_data), len(nega_data))\n",
    "    if (len(posi_data) < len(nega_data)):\n",
    "        nega_data = random.sample(nega_data, max(1, len(posi_data)))\n",
    "    train_data = TextDataset(posi_data + nega_data)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler = train_sampler, drop_last = False)#, num_workers = 4)\n",
    "    return train_dataloader\n",
    "\n",
    "for i in range(args.topic_number):\n",
    "    args.current_topic = i\n",
    "    model = Model(pretrain_model, config)\n",
    "    model.to(args.device)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = args.learning_rate)\n",
    "    \n",
    "    for epoch_num in range(args.epochs):\n",
    "        step = 0\n",
    "        for t in range(5):\n",
    "            train_dataloader = get_dataloader()\n",
    "            bar = tqdm(train_dataloader, total = len(train_dataloader))\n",
    "            for data in bar:\n",
    "                code_ids, position_ids, nodes, edges, labels = data\n",
    "                code_ids = code_ids.to(args.device)\n",
    "                position_ids = position_ids.to(args.device)\n",
    "                nodes = nodes.to(args.device)\n",
    "                edges = [[b.item() for b in a] for a in edges]\n",
    "                labels = labels.to(args.device)\n",
    "                model.train()\n",
    "                _, loss = model(code_ids, position_ids, nodes, edges, labels)\n",
    "                if args.n_gpu > 1:\n",
    "                    loss = loss.mean()\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "                bar.set_description(\"topic {} epoch {}\".format(i, epoch_num))\n",
    "                step += 1\n",
    "                if (step % args.gradient_accumulation_steps == 0):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "        if (step % args.gradient_accumulation_steps != 0):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        evaluate(model, epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heyuxuan",
   "language": "python",
   "name": "heyuxuan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
