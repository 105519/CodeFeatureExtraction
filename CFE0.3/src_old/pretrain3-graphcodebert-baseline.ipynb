{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4, 5'\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizer\n",
    "from transformers import WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arguments(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.total_length = 512\n",
    "args.graph_length = 0\n",
    "args.epochs = 1\n",
    "args.train_batch_size = 16\n",
    "args.eval_batch_size = 16\n",
    "\n",
    "args.gradient_accumulation_steps = 1\n",
    "args.max_grad_norm = 1.0\n",
    "args.learning_rate = 5e-5\n",
    "args.weight_decay = 0.0\n",
    "args.adam_epsilon = 1e-8\n",
    "\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.seed = 978"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "set_seed()\n",
    "config = RobertaConfig.from_pretrained('microsoft/graphcodebert-base')\n",
    "config.num_labels = 1\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/graphcodebert-base')\n",
    "model = RobertaForMaskedLM.from_pretrained('microsoft/graphcodebert-base', config = config)\n",
    "model.to(args.device)\n",
    "if args.n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename) as f:\n",
    "        text = []\n",
    "        for line in f:\n",
    "            text.append(line.strip())\n",
    "        dataset = []\n",
    "        for x in tqdm(text, total=len(text)):\n",
    "            dataset.append(eval(x))\n",
    "    return dataset\n",
    "\n",
    "dataset = read_data('../data/py150/washed_python150k.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    def __init__(self, code_ids, position_idx, edges, cross_edges):\n",
    "        self.code_ids = code_ids\n",
    "        self.position_idx = position_idx\n",
    "        self.edges = edges\n",
    "        self.cross_edges = cross_edges\n",
    "\n",
    "def convert_example_to_feature(example):\n",
    "    tokens = example['tokens']\n",
    "    nodes = example['nodes']\n",
    "    edges = example['edges']\n",
    "    cross_edges = example['cross_edges']\n",
    "\n",
    "    code_length = args.total_length - min(args.graph_length, len(nodes)) - 3\n",
    "    tokens = tokens[: code_length] \n",
    "    tokens = [tokenizer.tokenize(tokens[0])] \\\n",
    "           + [tokenizer.tokenize('@ ' + x)[1 :] for x in tokens[1 :]]\n",
    "    ori2cur_pos = {-1 : (0, 0)}\n",
    "    for i in range(len(tokens)):\n",
    "        ori2cur_pos[i] = (ori2cur_pos[i - 1][1], ori2cur_pos[i - 1][1] + len(tokens[i]))\n",
    "    tokens=[y for x in tokens for y in x] \n",
    "\n",
    "    #truncating\n",
    "    tokens = tokens[: code_length]\n",
    "    nodes = nodes[: args.graph_length]\n",
    "    edges = [(a, b) for (a, b) in edges if (a < len(nodes)) and (b < len(nodes))]\n",
    "    cross_edges = [(ori2cur_pos[a], b) for (a, b) in cross_edges\\\n",
    "                   if (a in ori2cur_pos) and (ori2cur_pos[a][1] < len(tokens)) and (b < len(nodes))]\n",
    "\n",
    "    #adding code tokens\n",
    "    code_tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
    "    code_ids = tokenizer.convert_tokens_to_ids(code_tokens)\n",
    "    position_idx = [i + tokenizer.pad_token_id + 1 for i in range(len(code_tokens))]\n",
    "\n",
    "    #adding graph nodes\n",
    "    code_tokens += [x for x in nodes]\n",
    "    code_ids += [tokenizer.unk_token_id] * len(nodes)\n",
    "    position_idx += [0] * len(nodes)\n",
    "    assert(len(code_ids) == len(position_idx))\n",
    "    assert(len(code_ids) < args.total_length)\n",
    "\n",
    "    #padding\n",
    "    padding_length = args.total_length - len(code_ids)\n",
    "    code_ids += [tokenizer.pad_token_id] * padding_length\n",
    "    position_idx += [tokenizer.pad_token_id] * padding_length\n",
    "    return InputFeatures(code_ids, position_idx, edges, cross_edges)\n",
    "\n",
    "def convert_examples_to_features(examples):\n",
    "    features = []\n",
    "    pool = multiprocessing.Pool(processes = 24)\n",
    "    for example in examples:\n",
    "        features.append(pool.apply_async(convert_example_to_feature, (example, )))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    for i in range(len(features)):\n",
    "        features[i] = features[i].get()\n",
    "    return features\n",
    "\n",
    "# train_examples = dataset[: int(len(dataset) * 0.67)]\n",
    "# eval_examples = dataset[int(len(dataset) * 0.67) :]\n",
    "# train_features = convert_examples_to_features(train_examples)\n",
    "# eval_features = convert_examples_to_features(eval_examples)\n",
    "train_features = convert_examples_to_features(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        attn_mask = np.zeros((args.total_length, args.total_length), dtype = np.bool)\n",
    "        node_index = sum([i > 1 for i in self.examples[item].position_idx])\n",
    "        max_length = sum([i != 1 for i in self.examples[item].position_idx])\n",
    "        \n",
    "        attn_mask[: node_index, : node_index] = True\n",
    "        for i, x in enumerate(self.examples[item].code_ids):\n",
    "            if x in [tokenizer.cls_token_id, tokenizer.sep_token_id]:\n",
    "                attn_mask[i, 0 : max_length] = True # [cls/sep, all]\n",
    "#                 attn_mask[0 : max_length, i] = True # test [all, cls/sep]\n",
    "        attn_mask[1 : node_index - 1, node_index] = True # cross edge (token, graph ROOT)\n",
    "        attn_mask[node_index, 1 : node_index - 1] = True # cross edge (graph ROOT, token)\n",
    "        for ((a, b), c) in self.examples[item].cross_edges:\n",
    "            attn_mask[a + 1 : b + 1, node_index + c] = True # cross edge (token, graph node)\n",
    "            attn_mask[node_index + c, a + 1 : b + 1] = True # cross edge (token, graph node)\n",
    "        for (a, b) in self.examples[item].edges:\n",
    "            attn_mask[node_index + a, node_index + b] = True # edge (source, target)\n",
    "#             attn_mask[node_index + b, node_index + a] = True # test (target, source)\n",
    "\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "        for x in self.examples[item].code_ids:\n",
    "            if (x in [tokenizer.cls_token_id, tokenizer.sep_token_id,\n",
    "                      tokenizer.unk_token_id, tokenizer.pad_token_id]):\n",
    "                input_ids.append(x)\n",
    "                labels.append(-100)\n",
    "            elif (random.randint(0, 99) < 15):\n",
    "                input_ids.append(tokenizer.mask_token_id)\n",
    "                labels.append(x)\n",
    "            else:\n",
    "                input_ids.append(x)\n",
    "                labels.append(-100)\n",
    "\n",
    "        return (torch.tensor(input_ids),\n",
    "                torch.tensor(self.examples[item].position_idx),\n",
    "                torch.tensor(attn_mask),\n",
    "                torch.tensor(labels))\n",
    "\n",
    "train_data = TextDataset(train_features)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, drop_last = True,\n",
    "                              batch_size = args.train_batch_size, num_workers = 4)\n",
    "# eval_data = TextDataset(eval_features)\n",
    "# eval_sampler = RandomSampler(eval_data)\n",
    "# eval_dataloader = DataLoader(eval_data, sampler = eval_sampler, shuffle = False, drop_last = False,\n",
    "#                              batch_size = args.eval_batch_size, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate():\n",
    "#     bar = tqdm(eval_dataloader, total = len(eval_dataloader))\n",
    "#     total = 0\n",
    "#     correct = 0\n",
    "#     model.eval()\n",
    "#     for batch in bar:\n",
    "#         (input_ids, position_ids, attention_mask, labels) = [x.to(args.device) for x in batch]\n",
    "#         with torch.no_grad():\n",
    "#             output = model(input_ids = input_ids,\n",
    "#                            position_ids = position_ids,\n",
    "#                            attention_mask = attention_mask)\n",
    "#         _, predicted = torch.max(output.logits, 2)\n",
    "#         predicted = predicted.view(1, -1).squeeze()\n",
    "#         labels = labels.view(1, -1).squeeze()\n",
    "#         total += (labels != -100).sum().item()\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "#     return correct / total\n",
    "# print(round(evaluate() * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.max_steps = args.epochs * len(train_dataloader)\n",
    "args.save_steps = len(train_dataloader) // 10\n",
    "args.warmup_steps = args.max_steps // 5\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': args.weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr = args.learning_rate, eps = args.adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = args.warmup_steps,\n",
    "                                            num_training_steps = args.max_steps)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "for epoch_id in range(args.epochs): \n",
    "    train_num = 0\n",
    "    train_loss = 0\n",
    "    avg_loss = 0\n",
    "    bar = tqdm(train_dataloader, total = len(train_dataloader))\n",
    "    \n",
    "    for step, batch in enumerate(bar):\n",
    "        (input_ids, position_ids, attention_mask, labels) = [x.to(args.device) for x in batch]\n",
    "        model.train()\n",
    "        output = model(input_ids = input_ids,\n",
    "                       position_ids = position_ids,\n",
    "                       attention_mask = attention_mask,\n",
    "                       labels = labels)\n",
    "        loss = output.loss\n",
    "\n",
    "        if args.n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "        if args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "        train_num += 1\n",
    "        train_loss += loss.item()\n",
    "        avg_loss = round(train_loss / train_num, 2)\n",
    "        bar.set_description(\"{}: loss {}\".format(epoch_id, avg_loss))\n",
    "\n",
    "        if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            \n",
    "#     print(evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '../saved-model/pretrain'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "torch.save(model.state_dict(), save_dir + '/3-graphcodebert-baseline.bin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
