{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizer\n",
    "from transformers import WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
    "from parser import remove_comments_and_docstrings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizer\n",
    "from transformers import WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arguments(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.total_length = 512\n",
    "args.graph_length = 0\n",
    "args.epochs = 6\n",
    "args.topic_size = 18\n",
    "\n",
    "# args.train_batch_size = 1\n",
    "# args.eval_batch_size = 1\n",
    "\n",
    "args.gradient_accumulation_steps = 1\n",
    "args.max_grad_norm = 1.0\n",
    "args.learning_rate = 5e-5\n",
    "args.weight_decay = 0.0\n",
    "args.adam_epsilon = 1e-8\n",
    "\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.seed = 978438233\n",
    "\n",
    "def set_seed():\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained('microsoft/codebert-base')\n",
    "config.num_labels = 1\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "model0 = RobertaForMaskedLM.from_pretrained('microsoft/codebert-base', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_tokens(code, do_remove):\n",
    "#     if (do_remove):\n",
    "#         code = remove_comments_and_docstrings(code, 'python')\n",
    "#     output_file = open('tmp' + '.py', 'w')\n",
    "#     print(code, file = output_file)\n",
    "#     output_file.close()\n",
    "    \n",
    "#     tokens = []\n",
    "#     f = open('tmp' + '.py', 'rb')\n",
    "#     tokenGenerator = tokenize.tokenize(f.readline)\n",
    "#     for token in tokenGenerator:\n",
    "#         if (token.type in [0, 59, 60, 62]): # COMMENT\n",
    "#             pass\n",
    "#         elif (token.type in [4, 61]): # NEWLINE\n",
    "#             pass\n",
    "#         elif (token.type == 5): # INDENT\n",
    "#             pass\n",
    "#         elif (token.type == 6): # DEDENT\n",
    "#             pass\n",
    "#         elif (token.type in [1, 2, 3, 54]): # NAME NUMBER STRING OP\n",
    "#             tokens.append(token.string)\n",
    "#         else:\n",
    "#             assert(False)\n",
    "#     f.close()\n",
    "#     return tokens\n",
    "\n",
    "# def search(path):\n",
    "#     data = []\n",
    "#     if (os.path.isdir(path)):\n",
    "#         for filename in os.listdir(path):\n",
    "#             data.extend(search(path + '/' + filename))\n",
    "#     else:\n",
    "#         assert(os.path.isfile(path))\n",
    "#         input_file = open(path, 'r')\n",
    "#         code = input_file.read()\n",
    "#         try:\n",
    "#             tokens = get_tokens(code, True)\n",
    "#         except:\n",
    "#             tokens = get_tokens(code, False)\n",
    "#         if (len(tokens) != 0):\n",
    "#             tokens = [tokenizer.tokenize(tokens[0])] \\\n",
    "#                    + [tokenizer.tokenize('@ ' + x)[1 :] for x in tokens[1 :]]\n",
    "#             tokens = [y for x in tokens for y in x]\n",
    "#             tokens = tokens[: args.total_length - 2]\n",
    "            \n",
    "#             code_ids = [tokenizer.cls_token_id] + tokenizer.convert_tokens_to_ids(tokens)\n",
    "#             position_ids = [i + tokenizer.pad_token_id + 1 for i in range(len(code_ids))]\n",
    "#             padding_length = args.total_length - len(code_ids)\n",
    "#             code_ids += [tokenizer.pad_token_id] * padding_length\n",
    "#             position_ids += [tokenizer.pad_token_id] * padding_length\n",
    "            \n",
    "#             data.append((code_ids, position_ids))\n",
    "#         input_file.close()\n",
    "#     return data\n",
    "\n",
    "# topic_map = dict()\n",
    "\n",
    "# def read_data(repo_file, topic_file):\n",
    "#     dataset = []\n",
    "#     f = open(repo_file, 'r')\n",
    "#     repos = []\n",
    "#     for line in f:\n",
    "#         _, repo = line.strip().split(chr(9))\n",
    "#         repo = repo[repo.rfind('/', 0, repo.rfind('/') - 1) + 1 :]\n",
    "#         repos.append(repo)\n",
    "#     f.close()\n",
    "#     f = open(topic_file, 'r')\n",
    "#     topics = json.loads(f.readline())['repos']\n",
    "#     f.close()\n",
    "    \n",
    "#     for x in topics:\n",
    "#         for y in x:\n",
    "#             if (y not in topic_map):\n",
    "#                 topic_map[y] = len(topic_map)\n",
    "    \n",
    "#     for repo, topic in tqdm(zip(repos, topics), total = len(repos)):\n",
    "#         labels = [0] * len(topic_map)\n",
    "#         for x in topic:\n",
    "#             labels[topic_map[x]] = 1\n",
    "#         data = search('../data/py150_files/' + repo)\n",
    "#         dataset.append(([x for x, y in data], [y for x, y in data], labels))\n",
    "#     return dataset\n",
    "\n",
    "# dataset = read_data('../data/py150/github_repos.txt', '../data/py150/repo_topics2.jsonl')\n",
    "# dataset = [x for x in dataset if (len(x[0]) != 0)]\n",
    "# print('get', len(dataset), 'datas in total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('dataset.jsonl', 'r')\n",
    "dataset0 = json.loads(f.readline())['dataset']\n",
    "f.close()\n",
    "\n",
    "topic_map = dict()\n",
    "\n",
    "def read_data(repo_file, topic_file):\n",
    "    f = open(repo_file, 'r')\n",
    "    repos = []\n",
    "    for line in f:\n",
    "        _, repo = line.strip().split(chr(9))\n",
    "        repo = repo[repo.rfind('/', 0, repo.rfind('/') - 1) + 1 :]\n",
    "        repos.append(repo)\n",
    "    f.close()\n",
    "    f = open(topic_file, 'r')\n",
    "    topics = json.loads(f.readline())['repos']\n",
    "    f.close()\n",
    "    \n",
    "    for x in topics:\n",
    "        for y in x:\n",
    "            if (y not in topic_map):\n",
    "                topic_map[y] = len(topic_map)\n",
    "    dataset = dataset0\n",
    "    for idx, topic in enumerate(topics):\n",
    "        labels = [0] * len(topic_map)\n",
    "        for x in topic:\n",
    "            labels[topic_map[x]] = 1\n",
    "        dataset[idx].append(labels)\n",
    "    return dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        data = self.examples[item]\n",
    "        code_ids, position_ids, labels = data\n",
    "        return torch.tensor(code_ids), torch.tensor(position_ids), torch.tensor(labels)\n",
    "\n",
    "dataset = read_data('../data/py150/github_repos.txt', '../data/py150/repo_topics2.jsonl')\n",
    "dataset = [x for x in dataset if (len(x[0]) != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        data = self.examples[item]\n",
    "        code_ids, position_ids, labels = data\n",
    "        return torch.tensor(code_ids), torch.tensor(position_ids), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TOPIC]: flask 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1309/1309 [04:28<00:00,  4.88it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:10<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 24 / 2 : 0.92\n",
      "negative: 220 / 1859 : 0.11\n",
      "f1: 0.025144054478784705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1309/1309 [04:29<00:00,  4.87it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:10<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 17 / 9 : 0.65\n",
      "negative: 1886 / 193 : 0.91\n",
      "f1: 0.1440677966101695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1309/1309 [04:28<00:00,  4.87it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:10<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 22 / 4 : 0.85\n",
      "negative: 1919 / 160 : 0.92\n",
      "f1: 0.21153846153846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1309/1309 [04:29<00:00,  4.87it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:10<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 20 / 6 : 0.77\n",
      "negative: 1935 / 144 : 0.93\n",
      "f1: 0.2105263157894737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1309/1309 [04:29<00:00,  4.87it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:10<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 22 / 4 : 0.85\n",
      "negative: 1919 / 160 : 0.92\n",
      "f1: 0.21153846153846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1309/1309 [04:28<00:00,  4.87it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:10<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 22 / 4 : 0.85\n",
      "negative: 1894 / 185 : 0.91\n",
      "f1: 0.1888412017167382\n",
      "\n",
      "[TOPIC]: library 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1003/1003 [03:25<00:00,  4.87it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:38<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 6 / 9 : 0.4\n",
      "negative: 1387 / 703 : 0.66\n",
      "f1: 0.01657458563535912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1003/1003 [03:25<00:00,  4.87it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:39<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 1 / 14 : 0.07\n",
      "negative: 1963 / 127 : 0.94\n",
      "f1: 0.013986013986013986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1003/1003 [03:26<00:00,  4.87it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:38<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 7 / 8 : 0.47\n",
      "negative: 1641 / 449 : 0.79\n",
      "f1: 0.029723991507430995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1003/1003 [03:25<00:00,  4.87it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:38<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 4 / 11 : 0.27\n",
      "negative: 1742 / 348 : 0.83\n",
      "f1: 0.021798365122615806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1003/1003 [03:25<00:00,  4.87it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:39<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 0 / 15 : 0.0\n",
      "negative: 2060 / 30 : 0.99\n",
      "f1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1003/1003 [03:26<00:00,  4.85it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:37<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 13 / 2 : 0.87\n",
      "negative: 400 / 1690 : 0.19\n",
      "f1: 0.015133876600698487\n",
      "\n",
      "[TOPIC]: testing 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 731/731 [02:26<00:00,  4.99it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [07:05<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 12 / 3 : 0.8\n",
      "negative: 395 / 1695 : 0.19\n",
      "f1: 0.013937282229965159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 731/731 [02:26<00:00,  4.98it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [07:05<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 14 / 1 : 0.93\n",
      "negative: 239 / 1851 : 0.11\n",
      "f1: 0.014893617021276595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 731/731 [02:26<00:00,  4.99it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [07:06<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 11 / 4 : 0.73\n",
      "negative: 977 / 1113 : 0.47\n",
      "f1: 0.01931518876207199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 731/731 [02:26<00:00,  4.99it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [07:05<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 12 / 3 : 0.8\n",
      "negative: 836 / 1254 : 0.4\n",
      "f1: 0.018735362997658083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 731/731 [02:26<00:00,  4.98it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [07:05<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 0 / 15 : 0.0\n",
      "negative: 2047 / 43 : 0.98\n",
      "f1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 731/731 [02:26<00:00,  4.98it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [07:05<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 15 / 0 : 1.0\n",
      "negative: 178 / 1912 : 0.09\n",
      "f1: 0.015447991761071062\n",
      "\n",
      "[TOPIC]: javascript 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 340/340 [01:08<00:00,  4.95it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:16<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 0 / 6 : 0.0\n",
      "negative: 2018 / 81 : 0.96\n",
      "f1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 340/340 [01:08<00:00,  4.95it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:16<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 6 / 0 : 1.0\n",
      "negative: 102 / 1997 : 0.05\n",
      "f1: 0.005973120955699352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 340/340 [01:08<00:00,  4.95it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:16<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 6 / 0 : 1.0\n",
      "negative: 326 / 1773 : 0.16\n",
      "f1: 0.0067226890756302525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 340/340 [01:08<00:00,  4.96it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:16<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 6 / 0 : 1.0\n",
      "negative: 610 / 1489 : 0.29\n",
      "f1: 0.007994670219853431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 340/340 [01:08<00:00,  4.95it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:16<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 6 / 0 : 1.0\n",
      "negative: 809 / 1290 : 0.39\n",
      "f1: 0.009216589861751152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 340/340 [01:08<00:00,  4.95it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:16<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 6 / 0 : 1.0\n",
      "negative: 885 / 1214 : 0.42\n",
      "f1: 0.009787928221859707\n",
      "\n",
      "[TOPIC]: machine-learning 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 986/986 [03:23<00:00,  4.85it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:13<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 1 / 26 : 0.04\n",
      "negative: 1985 / 93 : 0.96\n",
      "f1: 0.01652892561983471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 986/986 [03:23<00:00,  4.86it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:13<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 25 / 2 : 0.93\n",
      "negative: 588 / 1490 : 0.28\n",
      "f1: 0.03242542153047989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 986/986 [03:23<00:00,  4.85it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:13<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 24 / 3 : 0.89\n",
      "negative: 381 / 1697 : 0.18\n",
      "f1: 0.027459954233409613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 986/986 [03:22<00:00,  4.86it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:13<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 22 / 5 : 0.81\n",
      "negative: 733 / 1345 : 0.35\n",
      "f1: 0.03156384505021521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 986/986 [03:23<00:00,  4.85it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:13<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 23 / 4 : 0.85\n",
      "negative: 930 / 1148 : 0.45\n",
      "f1: 0.038397328881469114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 986/986 [03:23<00:00,  4.85it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:13<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 25 / 2 : 0.93\n",
      "negative: 607 / 1471 : 0.29\n",
      "f1: 0.03282994090610637\n",
      "\n",
      "[TOPIC]: data 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1615/1615 [05:34<00:00,  4.83it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:42<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 35 / 3 : 0.92\n",
      "negative: 252 / 1815 : 0.12\n",
      "f1: 0.037076271186440676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1615/1615 [05:34<00:00,  4.83it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:42<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 29 / 9 : 0.76\n",
      "negative: 1037 / 1030 : 0.5\n",
      "f1: 0.0528714676390155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1615/1615 [05:34<00:00,  4.82it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:42<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 0 / 38 : 0.0\n",
      "negative: 2059 / 8 : 1.0\n",
      "f1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1615/1615 [05:34<00:00,  4.82it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:44<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 28 / 10 : 0.74\n",
      "negative: 965 / 1102 : 0.47\n",
      "f1: 0.04794520547945206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1615/1615 [05:34<00:00,  4.83it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:42<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 20 / 18 : 0.53\n",
      "negative: 1557 / 510 : 0.75\n",
      "f1: 0.0704225352112676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1615/1615 [05:34<00:00,  4.83it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [06:42<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 29 / 9 : 0.76\n",
      "negative: 929 / 1138 : 0.45\n",
      "f1: 0.048132780082987554\n",
      "\n",
      "[TOPIC]: cli 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 629/629 [02:04<00:00,  5.05it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [05:46<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 0 / 9 : 0.0\n",
      "negative: 1953 / 143 : 0.93\n",
      "f1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 629/629 [02:04<00:00,  5.05it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [05:46<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 8 / 1 : 0.89\n",
      "negative: 335 / 1761 : 0.16\n",
      "f1: 0.008998875140607423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 629/629 [02:04<00:00,  5.04it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [05:46<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 1 / 8 : 0.11\n",
      "negative: 1918 / 178 : 0.92\n",
      "f1: 0.010638297872340427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 629/629 [02:04<00:00,  5.04it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [05:46<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 6 / 3 : 0.67\n",
      "negative: 216 / 1880 : 0.1\n",
      "f1: 0.0063324538258575196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 629/629 [02:04<00:00,  5.05it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [05:46<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 2 / 7 : 0.22\n",
      "negative: 1289 / 807 : 0.61\n",
      "f1: 0.0048899755501222485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 629/629 [02:04<00:00,  5.05it/s]\n",
      "100%|███████████████████████████████████████| 2105/2105 [05:46<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 5 / 4 : 0.56\n",
      "negative: 610 / 1486 : 0.29\n",
      "f1: 0.006666666666666667\n",
      "\n",
      "[TOPIC]: asyncio 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 459/459 [01:37<00:00,  4.72it/s]\n",
      "  3%|█                                        | 54/2105 [00:13<08:51,  3.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4805/2224789630.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mtest_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mmy_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4805/2224789630.py\u001b[0m in \u001b[0;36mmy_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4805/2224789630.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0my_trues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4805/2224789630.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, code_ids, position_ids, labels)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             bert_output = self.encoder.roberta(inputs_embeds = code_embeddings[i].view(1, args.total_length, -1),\n\u001b[0m\u001b[1;32m     56\u001b[0m                                                position_ids = position_ids[i].view(1, -1))\n\u001b[1;32m     57\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         )\n\u001b[0;32m--> 835\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 )\n\u001b[1;32m    521\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    523\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     ):\n\u001b[0;32m--> 337\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    338\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/h10/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mtranspose_for_scores\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mnew_x_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_x_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_file = open('bertresult.txt', 'w')\n",
    "\n",
    "def evaluate(model):\n",
    "    logits = []\n",
    "    y_trues = []\n",
    "    for data in tqdm(test_dataloader, total = len(test_dataloader)):\n",
    "        code_ids, position_ids, labels = data\n",
    "        code_ids = code_ids.to(args.device)\n",
    "        position_ids = position_ids.to(args.device)\n",
    "        labels = labels[:, args.topic_num].to(args.device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prob = model(code_ids, position_ids)\n",
    "            logits.append(prob.view(-1).cpu().numpy())\n",
    "            y_trues.append(labels.view(-1).cpu().numpy())\n",
    "    logits = np.concatenate(logits, 0)\n",
    "    y_trues = np.concatenate(y_trues, 0)\n",
    "    y_preds = logits > 0.5\n",
    "    TP = FP = TN = FN = 0\n",
    "    for x, y in zip(y_trues > 0.5, y_preds):\n",
    "        if (x == 1 and y == 1):\n",
    "            TP += 1\n",
    "        if (x == 1 and y == 0):\n",
    "            FP += 1\n",
    "        if (x == 0 and y == 0):\n",
    "            TN += 1\n",
    "        if (x == 0 and y == 1):\n",
    "            FN += 1\n",
    "    print('positive:', TP, '/', FP, ':', round(TP / (TP + FP), 2))\n",
    "    print('negative:', TN, '/', FN, ':', round(TN / (TN + FN), 2))\n",
    "    print('f1:', float(f1_score(y_trues, y_preds)))\n",
    "    print('positive:', TP, '/', FP, ':', round(TP / (TP + FP), 2), file = output_file)\n",
    "    print('negative:', TN, '/', FN, ':', round(TN / (TN + FN), 2), file = output_file)\n",
    "    print('f1:', float(f1_score(y_trues, y_preds)), file = output_file)\n",
    "\n",
    "class Model(nn.Module):   \n",
    "    def __init__(self, encoder, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.config = config\n",
    "        self.rnn = nn.LSTM(config.hidden_size, config.hidden_size, 1)\n",
    "        \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, 2) # * len(topic_map)\n",
    "        \n",
    "    def forward(self, code_ids, position_ids, labels = None):\n",
    "        code_ids = code_ids.view(-1, args.total_length)\n",
    "        position_ids = position_ids.view(-1, args.total_length)\n",
    "        code_embeddings = self.encoder.roberta.embeddings.word_embeddings(code_ids)\n",
    "        \n",
    "        h = torch.randn(1, 1, self.config.hidden_size).to(args.device)\n",
    "        c = torch.randn(1, 1, self.config.hidden_size).to(args.device)\n",
    "        for i in range(code_embeddings.size(0)):\n",
    "            bert_output = self.encoder.roberta(inputs_embeds = code_embeddings[i].view(1, args.total_length, -1),\n",
    "                                               position_ids = position_ids[i].view(1, -1))\n",
    "            _, (h, c) = self.rnn(bert_output[0][:, 0, :].view(-1, 1, config.hidden_size), (h, c))\n",
    "        x = h[0]\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x).view(-1, 2)\n",
    "        prob = F.softmax(x, dim = 1)[:, 1]\n",
    "        if (labels is None):\n",
    "            return prob\n",
    "        else:\n",
    "            labels = labels.view(-1)\n",
    "            loss_function = CrossEntropyLoss()\n",
    "            loss = loss_function(x, labels)\n",
    "            return loss\n",
    "\n",
    "def my_train():\n",
    "    model = Model(model0, config)\n",
    "    model.to(args.device)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = args.learning_rate)\n",
    "    \n",
    "    for epoch_num in range(args.epochs):\n",
    "        train_dataset = []\n",
    "        cnt_positive = 0\n",
    "        cnt_negative = 0\n",
    "        for x in dataset[: int(len(dataset) * 0.75)]:\n",
    "            if (x[2][args.topic_num] == 1):\n",
    "                train_dataset.append(x)\n",
    "                cnt_positive += 1\n",
    "        for x in dataset[: int(len(dataset) * 0.75)]:\n",
    "            if (x[2][args.topic_num] == 0) and (cnt_negative < 16 * cnt_positive):\n",
    "                train_dataset.append(x)\n",
    "                cnt_negative += 1\n",
    "#         print('P, N =', cnt_positive, cnt_negative)\n",
    "        \n",
    "        train_data = TextDataset(train_dataset)\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler = train_sampler, drop_last = True, num_workers = 4) #TODO\n",
    "        \n",
    "        step = 0\n",
    "        \n",
    "        for data in tqdm(train_dataloader, total = len(train_dataloader)):\n",
    "            code_ids, position_ids, labels = data\n",
    "            code_ids = code_ids[:, : 12, :]\n",
    "            position_ids = position_ids[:, : 12, :]\n",
    "            code_ids = code_ids.to(args.device)\n",
    "            position_ids = position_ids.to(args.device)\n",
    "            labels = labels[:, args.topic_num].view(-1, 1).to(args.device)\n",
    "            model.train()\n",
    "            loss = model(code_ids, position_ids, labels)\n",
    "#             if args.n_gpu > 1: # TODO\n",
    "#                 loss = loss.mean()\n",
    "            if (labels[0, 0] == 1): # TODO\n",
    "                loss = loss / cnt_positive * (cnt_positive + cnt_negative)\n",
    "            else:\n",
    "                loss = loss / cnt_negative * (cnt_positive + cnt_negative)\n",
    "            loss.backward()\n",
    "            step += 1\n",
    "            if (step % 32 == 0):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        if (step % 32 != 0):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        evaluate(model)\n",
    "\n",
    "for i in range(7, args.topic_size):\n",
    "    for x in topic_map:\n",
    "        if (topic_map[x] == i):\n",
    "            print('\\n[TOPIC]:', x, i)\n",
    "            print('\\n[TOPIC]:', x, file = output_file)\n",
    "    args.topic_num = i\n",
    "    random.shuffle(dataset)\n",
    "    test_dataset = dataset[int(len(dataset) * 0.75) :]\n",
    "    test_data = TextDataset(test_dataset)\n",
    "    test_sampler = RandomSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler = test_sampler, drop_last = False, num_workers = 4) #TODO\n",
    "    my_train()\n",
    "\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
