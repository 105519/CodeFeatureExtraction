{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizer\n",
    "from transformers import WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
    "from parser import remove_comments_and_docstrings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizer\n",
    "from transformers import WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arguments(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.total_length = 512\n",
    "args.graph_length = 0\n",
    "args.epochs = 10\n",
    "\n",
    "args.topic_num = 12\n",
    "\n",
    "# args.train_batch_size = 1\n",
    "# args.eval_batch_size = 1\n",
    "\n",
    "args.gradient_accumulation_steps = 1\n",
    "args.max_grad_norm = 1.0\n",
    "args.learning_rate = 5e-5\n",
    "args.weight_decay = 0.0\n",
    "args.adam_epsilon = 1e-8\n",
    "\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.seed = 978438233\n",
    "\n",
    "def set_seed():\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained('microsoft/codebert-base')\n",
    "config.num_labels = 1\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "model0 = RobertaForMaskedLM.from_pretrained('microsoft/codebert-base', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 8422/8422 [1:06:11<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_tokens(code, do_remove, tmp_file_name):\n",
    "    if (do_remove):\n",
    "        code = remove_comments_and_docstrings(code, 'python')\n",
    "    output_file = open(tmp_file_name, 'w')\n",
    "    print(code, file = output_file)\n",
    "    output_file.close()\n",
    "    \n",
    "    tokens = []\n",
    "    f = open(tmp_file_name, 'rb')\n",
    "    tokenGenerator = tokenize.tokenize(f.readline)\n",
    "    for token in tokenGenerator:\n",
    "        if (token.type in [0, 59, 60, 62]): # COMMENT\n",
    "            pass\n",
    "        elif (token.type in [4, 61]): # NEWLINE\n",
    "            pass\n",
    "        elif (token.type == 5): # INDENT\n",
    "            pass\n",
    "        elif (token.type == 6): # DEDENT\n",
    "            pass\n",
    "        elif (token.type in [1, 2, 3, 54]): # NAME NUMBER STRING OP\n",
    "            tokens.append(token.string)\n",
    "        else:\n",
    "            assert(False)\n",
    "    f.close()\n",
    "    return tokens\n",
    "\n",
    "def search(path):\n",
    "    data = []\n",
    "    if (os.path.isdir(path)):\n",
    "        for filename in os.listdir(path):\n",
    "            data.extend(search(path + '/' + filename))\n",
    "    else:\n",
    "        assert(os.path.isfile(path))\n",
    "        input_file = open(path, 'r')\n",
    "        code = input_file.read()\n",
    "        tmp_file_name = path.replace('/', '---').replace('.', '') + '.py'\n",
    "        tmp_file_name = 'tmp2.py'\n",
    "        try:\n",
    "            tokens = get_tokens(code, True, tmp_file_name)\n",
    "        except:\n",
    "            tokens = get_tokens(code, False, tmp_file_name)\n",
    "        os.system('rm ./' + tmp_file_name)\n",
    "        if (len(tokens) != 0):\n",
    "            tokens = [tokenizer.tokenize(tokens[0])] \\\n",
    "                   + [tokenizer.tokenize('@ ' + x)[1 :] for x in tokens[1 :]]\n",
    "            tokens = [y for x in tokens for y in x]\n",
    "            tokens = tokens[: args.total_length - 2]\n",
    "            \n",
    "            code_ids = [tokenizer.cls_token_id] + tokenizer.convert_tokens_to_ids(tokens)\n",
    "            position_ids = [i + tokenizer.pad_token_id + 1 for i in range(len(code_ids))]\n",
    "            padding_length = args.total_length - len(code_ids)\n",
    "            code_ids += [tokenizer.pad_token_id] * padding_length\n",
    "            position_ids += [tokenizer.pad_token_id] * padding_length\n",
    "            \n",
    "            data.append((code_ids, position_ids))\n",
    "        input_file.close()\n",
    "    return data\n",
    "\n",
    "topic_map = dict()\n",
    "\n",
    "def read_data(repo_file, topic_file):\n",
    "    dataset = []\n",
    "    f = open(repo_file, 'r')\n",
    "    repos = []\n",
    "    for line in f:\n",
    "        _, repo = line.strip().split(chr(9))\n",
    "        repo = repo[repo.rfind('/', 0, repo.rfind('/') - 1) + 1 :]\n",
    "        repos.append(repo)\n",
    "    f.close()\n",
    "    \n",
    "#     repos = repos[2010:]\n",
    "#     repos = repos[:100] # TODO\n",
    "\n",
    "    for repo in tqdm(repos, total = len(repos)):\n",
    "        data = search('../data/py150_files/' + repo)\n",
    "        dataset.append(([x for x, y in data], [y for x, y in data]))\n",
    "    return dataset\n",
    "\n",
    "dataset = read_data('../data/py150/github_repos.txt', '../data/py150/repo_topics2.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('dataset.jsonl', 'w')\n",
    "json.dump({'dataset' : dataset}, fp = f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
